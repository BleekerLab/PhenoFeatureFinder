{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual for feature selection\n",
    "\n",
    "## loading the data\n",
    "\n",
    "You will load your data as an object of the MetabolitePhenotypeFeatureSelection class, so you can apply the funtions of this class to your data. Therefore, you will need to give the object a name to which you will refer when using these functions. In this example, I will call my object 'example'. An object of this class needs two datasets: one with the phenotypes per sample and one with the cleaned metabolic data. To specify the names of your data files and where to find them, you enter the path to your metabolic data at 'metabolome_csv=' and the phenotypic data at 'phenotype_csv=' as done in the example. Next, you specify the name of the column with the sample IDs in the phenotypic dataset at 'phenotype_sample_id='.\n",
    "\n",
    "Make sure both your datasets are in a .csv format, uses '_' instead of ' ' (so no spaces), and is in lower case (so no capitals). \n",
    "\n",
    "For the metabolic data, the first column of your data should contain the feature IDs (so the identifier of the metabolic feature) and the other columns should be the samples. The values in the dataframe should be positive or zero. Next, you specify the name of the column with the feature IDs at 'metabolome_feature_id_col='. Ideally, this dataset is the output file from the write_clean_metabolome_to_csv() function from the MetaboliteAnalysis class. Therefore, the example data in this manual is the output from 'metabolite_data_filtering_manual.ipynb'. So the top of your dataset should look something like this:\n",
    "\n",
    "| feature_id    | group1_1  | group1_2  | group1_3  | group1_4  | group2_1  | group2_2  | group2_3  | group2_4  | group3_1  | group3_2  | group3_3  | group3_4  | group4_1  | group4_2  | group4_3  | group4_4  |\n",
    "|---------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
    "| feature_1     | 0         | 0         | 0         | 0         | 1906      | 1586      | 1720      | 1216      | 0         | 132       | 0         | 0         | 3964      | 2620      | 2900      | 2304      |\n",
    "| feature_2     | 1282      | 1094      | 5042      | 2140      | 174       | 0         | 0         | 0         | 0         | 412       | 746       | 660       | 0         | 302       | 720       | 986       |\n",
    "\n",
    "\n",
    "For the phenotypic dataset, the first column should contain the sample IDs, which should be the same as the sample column names of the metabolic dataset. The second column should contain should contain the two phenotypes or classes which you have assigned to the samples. In the example data, the group1 and group2 samples have the phenotype 'resistant' and the group3 and group4 samples the phenotype 'susceptible'. So this dataset should look something like this:\n",
    "\n",
    "| sample_id | phenotype   | \n",
    "|-----------|-------------|\n",
    "| group1_1  | resistant   |\n",
    "| group1_2  | resistant   |\n",
    "| group1_3  | resistant   |\n",
    "| group1_4  | resistant   |\n",
    "| group2_1  | resistant   |\n",
    "| group2_2  | resistant   |\n",
    "| group2_3  | resistant   |\n",
    "| group2_4  | resistant   |\n",
    "| group3_1  | susceptible |\n",
    "| group3_2  | susceptible |\n",
    "| group3_3  | susceptible |\n",
    "| group3_4  | susceptible |\n",
    "| group4_1  | susceptible |\n",
    "| group4_2  | susceptible |\n",
    "| group4_3  | susceptible |\n",
    "| group4_4  | susceptible |\n",
    "\n",
    "\n",
    "First load the MetabolitePhenotypeFeatureSelection class. If your working with phloemfinder as a downloaded package, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from phloemfinder.feature_selection_using_ml import MetabolitePhenotypeFeatureSelection "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your working with phloemfinder as a local github clone, you will need to open 'feature_selection_using_ml.py' in phloemfinder/src/phloemfinder. In this file change line 26 from\n",
    "\n",
    "```\n",
    "from phloemfinder.utils import compute_metrics_classification \n",
    "```\n",
    "to\n",
    "```\n",
    "from utils import compute_metrics_classification \n",
    "```\n",
    "\n",
    "After doing so, safe the changes, close and re-open your code editor and use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lissydenkers/miniconda3/envs/sklearn/lib/python3.9/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src/phloemfinder/')\n",
    "\n",
    "from feature_selection_using_ml import MetabolitePhenotypeFeatureSelection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load your data as object of the MetaboliteAnalysis class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = MetabolitePhenotypeFeatureSelection(\n",
    "    metabolome_csv='./data_for_manuals/cleaned_metabolite_example_data.csv',    \n",
    "    phenotype_csv='./data_for_manuals/phenotype_example_data.csv',\n",
    "    metabolome_feature_id_col='feature_id', \n",
    "    phenotype_sample_id='sample_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we need to validate both datasets. \n",
    "\n",
    "First, we need to check if the metabolome input data doesn't contain any negative values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metabolome data validated.\n"
     ]
    }
   ],
   "source": [
    "example.validate_input_metabolome_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we validate the phenotypic data. Specify the name of the column with the phenotypes (or classes) with 'phenotype_class_coll='. For the example data, this column is called \"phenotype\", as you can see is specified below. \n",
    "\n",
    "This validation step checks if the column you specified is present in your dataset and if you used a binary phenotype in this column. So, for the example data this means that we used \"resistant\" and \"susceptible\" as phenotype options, but this could also be \"good\" and \"bad\", or \"red\" and \"blue\". You could even use numbers, as long as the phenotype is binary. Personally, I prefer a descriptive phenotype, so that I will still understand what it means when coming back to it after a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phenotype data validated.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example.validate_input_phenotype_df(phenotype_class_col='phenotype')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Now that we have loaded and validated all the data, it is time to start associating the phenotypes to the metabolic profiles. Because each dataset is different, the optimal Machine Learning pipeline is different for each dataset as well. \n",
    "\n",
    "### Baseline performance\n",
    "\n",
    "Before we can build the optimal pipeline for our dataset, we need to set a baseline for the performance. We'll do this by running a simple Random Forest that should only take a few seconds. This baseline gives an idea of the performance we can expect from the more intricate pipeline we'll build in the next step. The performance of that pipeline should be better than the baseline performance.\n",
    "\n",
    "The parameters for this function are:\n",
    "\n",
    "* kfold: integer, default=5 <br> Cross-validation strategy to mitigate split effects on small datasets. <br> Default is to use a 5-fold cross-validation. <br> Has to be between 3 and 10. <br> For more information, see https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "        \n",
    "* train_size: float or integer, default=0.8 <br> If float, should be between 0.5 and 1.0 and represent the proportion of the dataset to include in the train split. <br> If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n",
    "\n",
    "* random_state: integer, default=123 <br> Controls both the randomness of the train/test split samples used when building trees and the sampling of the features to consider when looking for the best split at each node.\n",
    "\n",
    "* scoring_metric: str, default='balanced_accuracy' <br> A valid scoring value for the performance of the model. <br> 'balanced accuracy' is the average of recall obtained on each class. <br> To get a complete list, type: <br> >> from sklearn.metrics import SCORERS <br> >> sorted(SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training a basic Random Forest model =======\n",
      "Average balanced_accuracy score on training data is: 65.000 % -/+ 37.40\n",
      "\n",
      "\n",
      "====== Performance on test data of the basic Random Forest model =======\n",
      "Average balanced_accuracy score on test data is: 100.000 %\n"
     ]
    }
   ],
   "source": [
    "example.get_baseline_performance(\n",
    "    kfold=5, \n",
    "    train_size=0.8,\n",
    "    random_state=123,\n",
    "    scoring_metric='balanced_accuracy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "With the baseline set, you can build the pipeline for the selection of interesting features. This function is a wrapper for the Auto Machine Learning package TPOT. TPOT assembles the best fitting Machine Learning pipeline with many options for preprocessors and models using genetic programming. For this function, we have selected four models:\n",
    "\n",
    "* DecisionTreeClassifier\n",
    "\n",
    "* RandomForestClassifier\n",
    "\n",
    "* GradientBoostingClassifier\n",
    "\n",
    "* XGBClassifier\n",
    "\n",
    "and 13 preprocessors: \n",
    "\n",
    "* Binarizer\n",
    "\n",
    "* FeatureAgglomeration\n",
    "\n",
    "* MaxAbsScaler\n",
    "\n",
    "* MinMaxScaler\n",
    "\n",
    "* Normalizer\n",
    "\n",
    "* Nystroem\n",
    "\n",
    "* PCA\n",
    "\n",
    "* PolynomialFeatures\n",
    "\n",
    "* RBFSampler\n",
    "\n",
    "* RobustScaler\n",
    "\n",
    "* StandardScaler\n",
    "\n",
    "* ZeroCount\n",
    "\n",
    "* OneHotEncoder\n",
    "\n",
    "as options for the pipeline. <br> For more information on TPOT, see: https://epistasislab.github.io/tpot/.\n",
    "\n",
    "The parameters for this function are:\n",
    "\n",
    "* class_of_interest: string <br> The name of the class of interest (sometimes also called \"positive class\"). <br> This class will be used to calculate a recall and a precision score as follows: <br> $$ Recall = TP / (TP + FN) $$ <br> $$ Precision = TP / (TP + FP) $$ <br> where TP = true positives, FP = false positives, and FN = false negatives. \n",
    "\n",
    "* scoring_metric: string, default='balanced accuracy' <br> The function used to evaluate the quality of a given pipeline for the classification problem. <br> The following built-in scoring functions can be used:\n",
    "\n",
    "  * 'accuracy'\n",
    "\n",
    "  * 'adjusted_rand_score'\n",
    "\n",
    "  * 'average_precision'\n",
    "\n",
    "  * 'balanced_accuracy' \n",
    "\n",
    "  * 'f1'\n",
    "\n",
    "  * 'f1_macro'\n",
    "\n",
    "  * 'f1_micro'\n",
    "\n",
    "  * 'f1_samples'\n",
    "\n",
    "  * 'f1_weighted'\n",
    "\n",
    "  * 'neg_log_loss' \n",
    "\n",
    "  * 'precision' etc. (suffixes apply as with ‘f1’)\n",
    "\n",
    "  * 'recall' etc. (suffixes apply as with ‘f1’) \n",
    "\n",
    "  * ‘jaccard’ etc. (suffixes apply as with ‘f1’)\n",
    "\n",
    "  * 'roc_auc'\n",
    "\n",
    "  * ‘roc_auc_ovr’\n",
    "\n",
    "  * ‘roc_auc_ovo’\n",
    "\n",
    "  * ‘roc_auc_ovr_weighted’\n",
    "  \n",
    "  * ‘roc_auc_ovo_weighted’ \n",
    "\n",
    "* kfolds: integer, default=3 <br> Cross-validation strategy to mitigate split effects on small datasets. <br> Default is 3-fold cross-validation. <br> Has to be between 3 and 10. <br> For more information, see https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "* train_size: float or integer, default=0.8 <br> If float, should be between 0.5 and 1.0 and represent the proportion of the dataset to include in the train split. <br> If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n",
    "\n",
    "* max_time_mins: integer, default=5 <br> The time in minutes TPOT can use to optimize the pipeline (in total). This setting will allow TPOT to run until the specified time has elapsed and then stops the optimization process.\n",
    "\n",
    "* max_eval_time_mins: float, default=1 <br> The time in minutes TPOT can use to evaluate a single pipeline. <br> This time has to be shorter than the 'max_time_mins' setting.\n",
    "\n",
    "* random_state: integer, default=123 <br> Controls both the randomness of the train/test split samples used when building trees and the sampling of the features to consider when looking for the best split at each node.\n",
    "    \n",
    "* n_permutations: integer, default=10 <br> Number of permutations used to compute feature importances from the best model using the scikit-learn permutation_importance() method.\n",
    "\n",
    "* export_best_pipeline: boolean, default=True <br> If True, the best fitting pipeline is exported as .py file. This allows for reuse of the pipeline on new datasets.\n",
    "\n",
    "* path_for_saving_pipeline: string, default=\"./best_fitting_pipeline.py\" <br> The path and filename of the best fitting pipeline to save. <br> The name must have a '.py' extension. \n",
    "  \n",
    "\n",
    "When running this function with your real data, it would be good to do a quick test run (for example for the duration of a coffee break), to make sure everything is correct. If it looks fine, you can change the settings to overnight (depending on the time at which you start the run and at what time you want to use your computer again; for example max_time_mins=900). \n",
    "\n",
    "The 'Performance of ML model on train data' output ranges between 0% and 100%, while the 'Performance of ML model on test data' output ranges between 0 and 1. In all cases, higher means better.\n",
    "\n",
    "If the model is performing much better on the train data than on the test data, it means the model is overfitted and therefore not robust. \n",
    "\n",
    "The most important value from the test data is the 'recall'. A high recall means a low false positive rate.\n",
    "\n",
    "While running this step, try to close most other windows on your computer so it can work as efficiently as possible in the time you gave it. Oh, and make sure it doesn't go to sleep, otherwise it won't be doing anything.\n",
    "\n",
    "For the example, we'll be running the function with the default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.12.0 of tpot is outdated. Version 0.12.2 was released Friday February 23, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                              \n",
      "Generation 1 - Current best internal CV score: 0.75\n",
      "                                                                              \n",
      "Generation 2 - Current best internal CV score: 0.8333333333333334\n",
      "                                                                              \n",
      "Generation 3 - Current best internal CV score: 0.8333333333333334\n",
      "                                                                              \n",
      "Generation 4 - Current best internal CV score: 0.8333333333333334\n",
      "                                                                              \n",
      "5.29 minutes have elapsed. TPOT will close down.                              \n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "                                                                              \n",
      "                                                                              \n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "                                                                              \n",
      "Best pipeline: RandomForestClassifier(StandardScaler(CombineDFs(input_matrix, input_matrix)), bootstrap=False, criterion=gini, max_features=0.05, min_samples_leaf=4, min_samples_split=7, n_estimators=1000)\n",
      "============ Performance of ML model on train data =============\n",
      "Train balanced_accuracy score 100.000 %\n",
      "\n",
      "\n",
      "============ Performance of ML model on test data =============\n",
      "                   value\n",
      "balanced_accuracy  0.750\n",
      "precision          0.667\n",
      "recall             1.000\n",
      "f1 score           0.800\n",
      "\n",
      "\n",
      "======== Computing Principal Components importances on the training set =======\n"
     ]
    }
   ],
   "source": [
    "example.search_best_model_with_tpot_and_compute_pc_importances(\n",
    "    class_of_interest='resistant',\n",
    "    scoring_metric='balanced_accuracy',\n",
    "    kfolds=3,\n",
    "    train_size=0.8,\n",
    "    max_time_mins=5,\n",
    "    max_eval_time_mins=1,\n",
    "    random_state=123,\n",
    "    n_permutations=10,\n",
    "    export_best_pipeline=True,\n",
    "    path_for_saving_pipeline=\"./best_fitting_pipeline.py\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After five minutes of training, we get a pipeline with a balanced accuracy score for the training data of 100% and for the test data of 0.75 (or 75%). This means that the pipeline performs better on the training data than on the test data and is thus overfitted on the training data. Although the precision is only 0.667 (meaning the false negative rate is quite high), the recall is 1, so there are no false positives. In this case, this means that this pipeline will only label a sample as 'resistant' (the class of interest) when it is indeed resistant, but will label some samples as 'susceptible' while they are actually resistant. A run of only five minutes is very short and allowing TPOT to run longer will improve the performance.\n",
    "\n",
    "During the run, the function of 'Current best internal CV score' per generation, showing the cross-validation score of the best scoring pipeline per generation. After the run, a table is printed with the performance scores of the best fitting model/pipeline.\n",
    "\n",
    "To have a look at the final pipeline, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;featureunion&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;functiontransformer-1&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function copy at 0x1009360d0&gt;)),\n",
       "                                                (&#x27;functiontransformer-2&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function copy at 0x1009360d0&gt;))])),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(bootstrap=False, max_features=0.05,\n",
       "                                        min_samples_leaf=4, min_samples_split=7,\n",
       "                                        n_estimators=1000, random_state=123))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;featureunion&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;functiontransformer-1&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function copy at 0x1009360d0&gt;)),\n",
       "                                                (&#x27;functiontransformer-2&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function copy at 0x1009360d0&gt;))])),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(bootstrap=False, max_features=0.05,\n",
       "                                        min_samples_leaf=4, min_samples_split=7,\n",
       "                                        n_estimators=1000, random_state=123))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">featureunion: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;functiontransformer-1&#x27;,\n",
       "                                FunctionTransformer(func=&lt;function copy at 0x1009360d0&gt;)),\n",
       "                               (&#x27;functiontransformer-2&#x27;,\n",
       "                                FunctionTransformer(func=&lt;function copy at 0x1009360d0&gt;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>functiontransformer-1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function copy at 0x1009360d0&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>functiontransformer-2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function copy at 0x1009360d0&gt;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(bootstrap=False, max_features=0.05, min_samples_leaf=4,\n",
       "                       min_samples_split=7, n_estimators=1000,\n",
       "                       random_state=123)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('featureunion',\n",
       "                 FeatureUnion(transformer_list=[('functiontransformer-1',\n",
       "                                                 FunctionTransformer(func=<function copy at 0x1009360d0>)),\n",
       "                                                ('functiontransformer-2',\n",
       "                                                 FunctionTransformer(func=<function copy at 0x1009360d0>))])),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(bootstrap=False, max_features=0.05,\n",
       "                                        min_samples_leaf=4, min_samples_split=7,\n",
       "                                        n_estimators=1000, random_state=123))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the results\n",
    "To deal with the strong correlation between the features that is characteristic for metabolic data, the data is first 'flattened' into principal components (PCs) by the search_best_model_with_tpot_and_compute_pc_importances() function. These PCs are then used as features in the pipeline. \n",
    "\n",
    "The first step to extract the interesting features from the created pipeline, is to get the important PCs. The PC that is the most important for the model, gets the highest variable importance (var_imp). Sometimes, there are only a few PCs with a mean_var_imp>0. Other times there are many, meaning you will have to decide on a threshold for which are interesting enough.\n",
    "\n",
    "You can print the variable importances with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      mean_var_imp  std_var_imp     perm0     perm1  perm2  perm3  perm4  \\\n",
      "pc                                                                         \n",
      "PC0       0.033333     0.040825  0.083333  0.083333    0.0    0.0    0.0   \n",
      "PC1       0.033333     0.040825  0.083333  0.083333    0.0    0.0    0.0   \n",
      "PC4       0.033333     0.040825  0.083333  0.083333    0.0    0.0    0.0   \n",
      "PC10      0.025000     0.038188  0.000000  0.083333    0.0    0.0    0.0   \n",
      "PC2       0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC3       0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC5       0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC6       0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC7       0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC8       0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC9       0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC11      0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC12      0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC13      0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC14      0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "PC15      0.000000     0.000000  0.000000  0.000000    0.0    0.0    0.0   \n",
      "\n",
      "         perm5  perm6     perm7     perm8  perm9  \n",
      "pc                                                \n",
      "PC0   0.083333    0.0  0.000000  0.083333    0.0  \n",
      "PC1   0.083333    0.0  0.000000  0.083333    0.0  \n",
      "PC4   0.083333    0.0  0.083333  0.000000    0.0  \n",
      "PC10  0.083333    0.0  0.000000  0.083333    0.0  \n",
      "PC2   0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC3   0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC5   0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC6   0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC7   0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC8   0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC9   0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC11  0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC12  0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC13  0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC14  0.000000    0.0  0.000000  0.000000    0.0  \n",
      "PC15  0.000000    0.0  0.000000  0.000000    0.0  \n"
     ]
    }
   ],
   "source": [
    "print(example.pc_importances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test data, you can see that PC0, PC1 and PC4 all have a mean variable importance of 0.033 and PC10 has a mean variable importance of 0.025.\n",
    "\n",
    "Once you know which PCs are interesting, you can extract the features most important for these PCs.\n",
    "\n",
    "**!!You have to be careful with the PC number here!!** In the list of PC importances above, the PC numbers start at 0. In this function, however, the PC numbers are +1 to prevent some errors. So if you are interested in PC3 from the list above, you should specify 'selected_pc=4' in this function.\n",
    "\n",
    "* selected_pc: integer, default=1 <br> The Principal Component of which you want to know the most important features. \n",
    "* top_n: integer, default=5 <br> Number of features to select. The top_n features with the highest absolute loadings will be selected from the selected_pc you specified. \n",
    "\n",
    "For the example data we'll take a look at the top 10 most important features of PC0 from the list (so we specify selected_pc=1 for this next function). Next to the names of the features, this function will also produce the loading scores. The higher this score, the more important the feature is for the selected PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the metabolite names with the top 10 absolute loadings on PC1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>loading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feature_8913</td>\n",
       "      <td>0.355209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feature_9321</td>\n",
       "      <td>0.311647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feature_5216</td>\n",
       "      <td>0.299312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature_8850</td>\n",
       "      <td>0.270446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feature_5162</td>\n",
       "      <td>0.196034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feature_3928</td>\n",
       "      <td>0.179552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feature_9803</td>\n",
       "      <td>0.176027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>feature_10785</td>\n",
       "      <td>0.175505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feature_8953</td>\n",
       "      <td>0.172160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>feature_2802</td>\n",
       "      <td>0.167416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_name   loading\n",
       "0   feature_8913  0.355209\n",
       "1   feature_9321  0.311647\n",
       "2   feature_5216  0.299312\n",
       "3   feature_8850  0.270446\n",
       "4   feature_5162  0.196034\n",
       "5   feature_3928  0.179552\n",
       "6   feature_9803  0.176027\n",
       "7  feature_10785  0.175505\n",
       "8   feature_8953  0.172160\n",
       "9   feature_2802  0.167416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.get_names_of_top_n_features_from_selected_pc(\n",
    "    selected_pc=1,\n",
    "    top_n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the top features from all interesting PCs, you can go back to your data to for example have a look at the abundance of the features per group, or annotate them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
